# Pipeline de CiÃªncia de Dados - AnÃ¡lise de CotaÃ§Ãµes

Este documento descreve o pipeline completo para usar os dados de cotaÃ§Ãµes em projetos de ciÃªncia de dados.

## ğŸ—ï¸ Arquitetura do Pipeline

```
Dados Parquet (ETL) â†’ AnÃ¡lise ExploratÃ³ria â†’ Feature Engineering â†’ Modelagem â†’ Deploy
     â†“                        â†“                      â†“              â†“          â†“
   Storage              Jupyter Notebooks        TransformaÃ§Ãµes   ML Models   API/Dashboard
```

## ğŸ“Š 1. CARREGAMENTO E EXPLORAÃ‡ÃƒO DE DADOS

### Estrutura de Projeto Recomendada:
```
projeto-ciencia-dados/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Link simbÃ³lico para os dados Parquet
â”‚   â”œâ”€â”€ processed/              # Dados processados
â”‚   â””â”€â”€ features/               # Features engineered
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-exploracao.ipynb    # AnÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ 02-feature-eng.ipynb   # Feature engineering
â”‚   â”œâ”€â”€ 03-modelagem.ipynb     # Modelos ML
â”‚   â””â”€â”€ 04-validacao.ipynb     # ValidaÃ§Ã£o e testes
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py          # Carregamento de dados
â”‚   â”‚   â””â”€â”€ preprocessor.py    # PrÃ©-processamento
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ builder.py         # ConstruÃ§Ã£o de features
â”‚   â”‚   â””â”€â”€ selector.py        # SeleÃ§Ã£o de features
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Treinamento
â”‚   â”‚   â”œâ”€â”€ predictor.py       # PrediÃ§Ãµes
â”‚   â”‚   â””â”€â”€ evaluator.py       # AvaliaÃ§Ã£o
â”‚   â””â”€â”€ visualization/
â”‚       â””â”€â”€ plots.py           # VisualizaÃ§Ãµes
â”œâ”€â”€ models/                     # Modelos salvos
â”œâ”€â”€ reports/                    # RelatÃ³rios gerados
â””â”€â”€ requirements.txt           # DependÃªncias
```

## ğŸš€ 2. QUICK START

### InstalaÃ§Ã£o:
```bash
# Instalar dependÃªncias bÃ¡sicas
pip install -r requirements.txt

# Instalar dependÃªncias completas para DS
pip install -r requirements-datascience.txt

# Iniciar Jupyter
jupyter notebook
```

### Carregamento RÃ¡pido:
```python
from src.data_science_loader import DataScience_Loader

# Inicializar
loader = DataScience_Loader("output/parquet")

# Carregar dados para anÃ¡lise
data = loader.load_time_series(
    start_date="2024-01-01",
    end_date="2025-08-27",
    table_type="indicadores_estados",
    states=["SÃ£o Paulo", "Minas Gerais"]  # Estados especÃ­ficos
)

print(f"Dados carregados: {len(data)} registros")
```

## ğŸ“ˆ 3. CASOS DE USO TÃPICOS

### A. **ğŸ”® PrevisÃ£o de PreÃ§os (Forecasting)**

**Objetivo**: Prever preÃ§os futuros do boi gordo  
**Horizonte**: 1-30 dias  
**Modelos**: ARIMA, Prophet, LSTM, XGBoost

```python
# Exemplo de setup para forecasting
from src.data_science_loader import load_for_forecasting

# Carregar dados otimizados
ts_data = load_for_forecasting(
    parquet_path="output/parquet",
    target_state="SÃ£o Paulo",
    lookback_days=365
)

# Usar com Prophet
from prophet import Prophet
model = Prophet()
model.fit(ts_data.rename(columns={'date': 'ds', 'price_brl': 'y'}))
```

**MÃ©tricas**: MAE, MAPE, RMSE  
**ValidaÃ§Ã£o**: Time series cross-validation

### B. **ğŸ“Š AnÃ¡lise de Volatilidade**

**Objetivo**: Modelar e prever volatilidade  
**Modelos**: GARCH, EGARCH, Stochastic Volatility

```python
# Exemplo de anÃ¡lise de volatilidade
import pandas as pd
from arch import arch_model

# Calcular retornos
returns = data['price_brl'].pct_change().dropna() * 100

# Modelo GARCH
model = arch_model(returns, vol='GARCH', p=1, q=1)
result = model.fit()
```

### C. **ğŸ—ºï¸ Clustering de Estados**

**Objetivo**: Agrupar estados por comportamento de preÃ§os  
**Algoritmos**: K-Means, Hierarchical Clustering

```python
# Matriz de correlaÃ§Ã£o entre estados
pivot_data = loader.create_pivot_table(
    data, value_col='price_brl', 
    columns_col='state'
)

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Clustering
scaler = StandardScaler()
scaled_data = scaler.fit_transform(pivot_data.T)
kmeans = KMeans(n_clusters=4)
clusters = kmeans.fit_predict(scaled_data)
```

### D. **âš–ï¸ AnÃ¡lise de Spread**

**Objetivo**: Identificar oportunidades de arbitragem  
**MÃ©trica**: DiferenÃ§as de preÃ§os entre regiÃµes

```python
# Calcular spreads
sp_prices = data[data['state'] == 'SÃ£o Paulo']['price_brl']
mg_prices = data[data['state'] == 'Minas Gerais']['price_brl']
spread = sp_prices - mg_prices

# Detectar oportunidades
opportunities = spread[abs(spread) > spread.std() * 2]
```

### E. **ğŸš¨ DetecÃ§Ã£o de Anomalias**

**Objetivo**: Identificar preÃ§os anÃ´malos automaticamente  
**Algoritmos**: Isolation Forest, DBSCAN, Statistical tests

```python
from sklearn.ensemble import IsolationForest

# Detectar anomalias
iso_forest = IsolationForest(contamination=0.1)
anomalies = iso_forest.fit_predict(data[['price_brl']])
```

## ğŸ¯ 4. FEATURES PARA MACHINE LEARNING

### Features Temporais:
```python
# Features automÃ¡ticas com o loader
features_df = loader.create_features_dataframe(
    data, price_col='price_brl'
)
```

- âœ… **Lags**: price_t-1, price_t-7, price_t-30
- âœ… **MÃ©dias MÃ³veis**: MA_7, MA_30, MA_90
- âœ… **Volatilidade**: Vol_7, Vol_30 (rolling std)
- âœ… **Retornos**: Return_1d, Return_7d
- âœ… **Sazonalidade**: month, quarter, day_of_week

### Features TÃ©cnicas AvanÃ§adas:
```python
def create_technical_indicators(data):
    # RSI
    data['RSI'] = calculate_rsi(data['price_brl'], window=14)
    
    # Bollinger Bands
    data['BB_upper'] = data['price_brl'].rolling(20).mean() + 2*data['price_brl'].rolling(20).std()
    data['BB_lower'] = data['price_brl'].rolling(20).mean() - 2*data['price_brl'].rolling(20).std()
    
    # MACD
    ema12 = data['price_brl'].ewm(span=12).mean()
    ema26 = data['price_brl'].ewm(span=26).mean()
    data['MACD'] = ema12 - ema26
    
    return data
```

### Features EconÃ´micas (externas):
- ğŸ’± **Taxa de cÃ¢mbio**: USD/BRL
- ğŸ“Š **Commodities**: Milho, Soja (correlacionadas)
- ğŸ“ˆ **Ãndices**: Bovespa, CDI, IPCA
- ğŸŒ **ExportaÃ§Ãµes**: Volume de carne bovina

## ğŸ”§ 5. STACK TECNOLÃ“GICA

### ğŸ—ï¸ **Infraestrutura Base**:
- **pandas**: ManipulaÃ§Ã£o de dados
- **numpy**: OperaÃ§Ãµes numÃ©ricas  
- **pyarrow**: Leitura eficiente de Parquet
- **jupyter**: AnÃ¡lise interativa

### ğŸ“Š **VisualizaÃ§Ã£o**:
- **matplotlib**: GrÃ¡ficos estÃ¡ticos
- **seaborn**: AnÃ¡lises estatÃ­sticas
- **plotly**: GrÃ¡ficos interativos
- **streamlit**: Dashboards web

### ğŸ¤– **Machine Learning**:
- **scikit-learn**: ML tradicional
- **xgboost**: Gradient boosting
- **tensorflow**: Deep learning
- **optuna**: Hyperparameter tuning

### â° **SÃ©ries Temporais**:
- **prophet**: Forecasting robusto
- **statsmodels**: Modelos estatÃ­sticos
- **arch**: Modelos GARCH
- **tsfresh**: Feature extraction automÃ¡tica

### âš¡ **Performance**:
- **dask**: Processamento paralelo
- **polars**: DataFrame ultra-rÃ¡pido
- **numba**: CompilaÃ§Ã£o JIT

## ğŸ“Š 6. PERFORMANCE E OTIMIZAÃ‡ÃƒO

### Para datasets grandes (10 anos):

#### OtimizaÃ§Ãµes de Carregamento:
```python
# âœ… Carregar apenas colunas necessÃ¡rias
data = loader.load_data_range(
    start_date="2020-01-01",
    end_date="2025-08-27",
    columns=["date", "state", "price_brl"],  # Apenas essenciais
    parallel=True  # Processamento paralelo
)

# âœ… Filtros pushdown (usar particionamento)
# âœ… Chunking para modelos grandes
# âœ… Cache de resultados intermediÃ¡rios
```

#### Memory Management:
```python
# Para datasets muito grandes
import dask.dataframe as dd

# Usar Dask para processamento out-of-core
ddf = dd.read_parquet("output/parquet/*/")
result = ddf.groupby("state").price_brl.mean().compute()
```

### Benchmarks Estimados:

| OperaÃ§Ã£o | 1 ano | 5 anos | 10 anos |
|----------|--------|--------|---------|
| Carregamento | 2s | 8s | 15s |
| EDA Completa | 30s | 2min | 5min |
| Feature Engineering | 10s | 45s | 2min |
| Modelo XGBoost | 5s | 30s | 1min |
| Forecasting Prophet | 15s | 1min | 3min |

**Hardware recomendado**:
- **RAM**: 16GB+ (32GB para 10 anos)
- **CPU**: 8+ cores
- **Storage**: SSD recomendado

## ğŸ¯ 7. WORKFLOWS TÃPICOS

### Workflow 1: **AnÃ¡lise ExploratÃ³ria RÃ¡pida**
```python
# 1. Carregar dados
data = loader.load_time_series("2024-01-01", "2025-08-27", "indicadores_estados")

# 2. EstatÃ­sticas bÃ¡sicas
print(data.describe())

# 3. VisualizaÃ§Ã£o temporal
data.groupby('date')['price_brl'].mean().plot(figsize=(12,6))

# 4. AnÃ¡lise por estado
data.groupby('state')['price_brl'].mean().sort_values().plot(kind='bar')
```

### Workflow 2: **Modelo de PrevisÃ£o Completo**
```python
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# 1. Preparar features
features_df = loader.create_features_dataframe(data)
features_df = features_df.dropna()

# 2. Split temporal
X = features_df.select_dtypes(include=[np.number]).drop(['price_brl'], axis=1)
y = features_df['price_brl']

tscv = TimeSeriesSplit(n_splits=5)

# 3. Treinar modelo
scores = []
for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    
    model = RandomForestRegressor()
    model.fit(X_train, y_train)
    pred = model.predict(X_val)
    scores.append(mean_absolute_error(y_val, pred))

print(f"MAE mÃ©dio: {np.mean(scores):.2f}")
```

### Workflow 3: **Dashboard Automatizado**
```python
import streamlit as st

# Dashboard com Streamlit
st.title("ğŸ‚ Dashboard de CotaÃ§Ãµes")

# Sidebar para filtros
states = st.sidebar.multiselect("Estados", data['state'].unique())
date_range = st.sidebar.date_input("PerÃ­odo", [start_date, end_date])

# Filtrar dados
filtered_data = data[
    (data['state'].isin(states)) & 
    (data.index >= date_range[0]) & 
    (data.index <= date_range[1])
]

# Plotar
st.line_chart(filtered_data.groupby('date')['price_brl'].mean())
```

## ğŸ¨ 8. TEMPLATES DE ANÃLISE

### Template: **AnÃ¡lise de Sazonalidade**
```python
def analyze_seasonality(data):
    # DecomposiÃ§Ã£o sazonal
    from statsmodels.tsa.seasonal import seasonal_decompose
    
    monthly_data = data.groupby(data.index.to_period('M'))['price_brl'].mean()
    decomposition = seasonal_decompose(monthly_data, model='additive')
    
    fig, axes = plt.subplots(4, 1, figsize=(15, 10))
    decomposition.observed.plot(ax=axes[0], title='Original')
    decomposition.trend.plot(ax=axes[1], title='TendÃªncia')
    decomposition.seasonal.plot(ax=axes[2], title='Sazonalidade')
    decomposition.resid.plot(ax=axes[3], title='Residual')
    
    return decomposition
```

### Template: **Backtesting de EstratÃ©gias**
```python
def backtest_strategy(data, strategy_func, initial_capital=100000):
    """
    Template para backtesting de estratÃ©gias
    """
    positions = strategy_func(data)  # 1 = comprar, -1 = vender, 0 = neutro
    returns = data['price_brl'].pct_change()
    strategy_returns = positions.shift(1) * returns
    
    cumulative_returns = (1 + strategy_returns).cumprod()
    final_value = initial_capital * cumulative_returns.iloc[-1]
    
    metrics = {
        'return': (final_value / initial_capital - 1) * 100,
        'sharpe': strategy_returns.mean() / strategy_returns.std() * np.sqrt(252),
        'max_drawdown': (cumulative_returns / cumulative_returns.expanding().max() - 1).min() * 100
    }
    
    return metrics, cumulative_returns
```

## ğŸš€ 9. DEPLOY E PRODUÃ‡ÃƒO

### OpÃ§Ã£o 1: **API REST com FastAPI**
```python
from fastapi import FastAPI
import joblib

app = FastAPI()
model = joblib.load('models/price_predictor.pkl')

@app.post("/predict")
async def predict_price(data: dict):
    prediction = model.predict([list(data.values())])
    return {"predicted_price": prediction[0]}
```

### OpÃ§Ã£o 2: **Streamlit Dashboard**
```bash
# Comando para deploy
streamlit run dashboard.py --server.port 8501
```

### OpÃ§Ã£o 3: **Jupyter Notebooks como Reports**
```python
# Usando papermill para automatizar notebooks
import papermill as pm

pm.execute_notebook(
    'template.ipynb',
    f'reports/report_{datetime.now().strftime("%Y-%m-%d")}.ipynb',
    parameters={'start_date': '2025-01-01', 'end_date': '2025-08-27'}
)
```

## ğŸ“‹ 10. CHECKLIST DE PROJETO

### âœ… **Setup Inicial**:
- [ ] Dados ETL coletados e validados
- [ ] Ambiente virtual configurado
- [ ] DependÃªncias instaladas
- [ ] Jupyter notebook funcionando

### âœ… **AnÃ¡lise ExploratÃ³ria**:
- [ ] EstatÃ­sticas descritivas
- [ ] VisualizaÃ§Ãµes temporais
- [ ] AnÃ¡lise por estados/regiÃµes
- [ ] IdentificaÃ§Ã£o de outliers
- [ ] PadrÃµes sazonais

### âœ… **Feature Engineering**:
- [ ] Features temporais (lags, mÃ©dias mÃ³veis)
- [ ] Indicadores tÃ©cnicos
- [ ] Features categÃ³ricas (estado, mÃªs)
- [ ] NormalizaÃ§Ã£o/escalonamento
- [ ] SeleÃ§Ã£o de features

### âœ… **Modelagem**:
- [ ] Baseline simples (mÃ©dia, Ãºltimo valor)
- [ ] Modelos tradicionais (linear, tree-based)
- [ ] SÃ©ries temporais (ARIMA, Prophet)
- [ ] Deep Learning (LSTM, GRU)
- [ ] Ensemble methods

### âœ… **ValidaÃ§Ã£o**:
- [ ] Split temporal correto
- [ ] Cross-validation adequada
- [ ] MÃ©tricas de negÃ³cio
- [ ] Backtesting para estratÃ©gias
- [ ] AnÃ¡lise de resÃ­duos

### âœ… **Deploy**:
- [ ] Modelo final selecionado
- [ ] Pipeline de inference
- [ ] API ou dashboard
- [ ] Monitoramento de performance
- [ ] DocumentaÃ§Ã£o

## ğŸ’¡ 11. DICAS E BOAS PRÃTICAS

### ğŸ¯ **AnÃ¡lise de Dados**:
1. **Sempre validar com dados out-of-time** (never future data)
2. **Considerar feriados e fins de semana** (dados podem nÃ£o existir)
3. **Documentar hipÃ³teses e descobertas** em markdown
4. **Usar visualizaÃ§Ãµes interativas** para exploraÃ§Ã£o
5. **Salvar datasets intermediÃ¡rios** para reuso

### âš¡ **Performance**:
1. **Carregar apenas dados necessÃ¡rios** (perÃ­odo + colunas)
2. **Usar processamento paralelo** quando possÃ­vel
3. **Cache resultados custosos** (feature engineering)
4. **Monitor memory usage** em datasets grandes
5. **Considerar downsampling** para visualizaÃ§Ãµes

### ğŸ”¬ **Modelagem**:
1. **ComeÃ§ar sempre com baseline simples**
2. **Feature importance** para interpretabilidade  
3. **Usar mÃ©tricas de negÃ³cio** alÃ©m de tÃ©cnicas
4. **ValidaÃ§Ã£o cruzada temporal** obrigatÃ³ria
5. **Documentar hyperparameters** e resultados

### ğŸš€ **Deploy**:
1. **Versionamento de modelos** (MLflow, DVC)
2. **Monitoring de drift** nos dados
3. **Alertas para anomalias** de performance
4. **Backup de modelos** anteriores
5. **Logs detalhados** de prediÃ§Ãµes

## ğŸ“ 12. SUPORTE E TROUBLESHOOTING

### â“ **Problemas Comuns**:

**"Dados nÃ£o carregam"**:
- âœ… Verificar se ETL foi executado
- âœ… Validar caminhos dos arquivos
- âœ… Conferir formato de datas

**"Notebook muito lento"**:
- âœ… Reduzir perÃ­odo de anÃ¡lise
- âœ… Carregar apenas colunas necessÃ¡rias  
- âœ… Usar `parallel=True`
- âœ… Considerar usar Dask

**"Modelos com performance ruim"**:
- âœ… Verificar data leakage
- âœ… Validar split temporal
- âœ… Adicionar mais features
- âœ… Tunar hyperparameters

**"MemÃ³ria insuficiente"**:
- âœ… Processar dados em chunks
- âœ… Usar dtypes otimizados
- âœ… Clear variables desnecessÃ¡rias
- âœ… Considerar Dask/Polars

### ğŸ†˜ **Getting Help**:
1. Verificar logs de erro detalhadamente
2. Conferir versÃµes das bibliotecas
3. Testar com dataset menor primeiro
4. Consultar documentaÃ§Ã£o das libs
5. Procurar no Stack Overflow

---

## ğŸ‰ CONCLUSÃƒO

Este pipeline oferece uma base sÃ³lida para anÃ¡lise de dados de cotaÃ§Ãµes, desde exploraÃ§Ã£o bÃ¡sica atÃ© modelos avanÃ§ados em produÃ§Ã£o. 

**PrÃ³ximos passos recomendados**:
1. Executar notebook de exploraÃ§Ã£o
2. Implementar seu primeiro modelo
3. Criar dashboard personalizado  
4. Automatizar anÃ¡lises regulares
5. Expandir para outras commodities

**Lembre-se**: Comece simples, valide constantemente e documente tudo! ğŸš€
